{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c66e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "任务: 合并json\n",
      "输入文件: E:\\Dissertation\\XGBoost_cleaning\\label_studio_config_central_new.json\n",
      "输出目录: E:\\Dissertation\\XGBoost_cleaning\n"
     ]
    }
   ],
   "source": [
    "# ========= Cell 1: 填写任务信息 =========\n",
    "from datetime import datetime\n",
    "import ipynbname\n",
    "import os\n",
    "\n",
    "task_name = \"合并json\"\n",
    "notebook_name = \"XGBoost.ipynb\"  # 不带扩展名\n",
    "notebook_path = r\"CASA0004\\XGBoost.ipynb\"  # 完整路径\n",
    "dataset = r\"E:\\Dissertation\\CASA0004\\central_barrio_features.csv\"\n",
    "code_version = \"v1.0_X, (1 part of xgboost)\"\n",
    "input_dir = r\"E:\\Dissertation\\XGBoost_cleaning\\label_studio_config_central_new.json\"   # 输入 CSV\n",
    "input_json = r\"E:\\Dissertation\\XGBoost_cleaning\\label_studio_config_central.json\"\n",
    "output_dir = r\"E:\\Dissertation\\XGBoost_cleaning\"       # 输出文件夹\n",
    "note = \"把csv里面的na全都填上0，然后训练xgboost预测谋杀率\"\n",
    "\n",
    "# 保证输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"任务: {task_name}\")\n",
    "print(f\"输入文件: {input_dir}\")\n",
    "print(f\"输出目录: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f6baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(input_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "# 两个列表拼接\n",
    "merged = data1 + data2\n",
    "\n",
    "with open(\"label_studio_config_central_merged.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283caa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m point_counts = defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     pid = \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33msampling_point_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUNKNOWN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     point_counts[pid] += \u001b[32m1\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 已出现的点（转为 int，方便排序和区间分析）\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'data'"
     ]
    }
   ],
   "source": [
    "#=============最早的能用代码===============#\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 输入 JSON 文件路径\n",
    "input_json = \"E:\\Dissertation\\CASA0004\\label_studio_config_central_merged.json\"\n",
    "\n",
    "# 总采样点数\n",
    "total_points = 34600  \n",
    "\n",
    "# 加载 JSON\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 统计每个 point 的图片数量\n",
    "point_counts = defaultdict(int)\n",
    "for item in data:\n",
    "    pid = item[\"data\"].get(\"sampling_point_id\", \"UNKNOWN\")\n",
    "    point_counts[pid] += 1\n",
    "\n",
    "# 已出现的点（转为 int，方便排序和区间分析）\n",
    "present_points = set(int(pid.split(\"_\")[1]) for pid in point_counts.keys() if pid.startswith(\"point_\"))\n",
    "\n",
    "# 所有应有点 ID\n",
    "all_points = set(range(1, total_points + 1))\n",
    "\n",
    "# 缺失点（完全没有出现的点）\n",
    "missing_points = sorted(all_points - present_points)\n",
    "\n",
    "# 计算缺失区间\n",
    "missing_intervals = []\n",
    "if missing_points:\n",
    "    start = missing_points[0]\n",
    "    prev = missing_points[0]\n",
    "    for p in missing_points[1:]:\n",
    "        if p == prev + 1:\n",
    "            prev = p\n",
    "        else:\n",
    "            missing_intervals.append((start, prev, prev - start + 1))  # (起点, 终点, 缺失数量)\n",
    "            start = p\n",
    "            prev = p\n",
    "    missing_intervals.append((start, prev, prev - start + 1))  # 收尾\n",
    "\n",
    "# 找出缺失最多的前 10 个区间\n",
    "missing_intervals_sorted = sorted(missing_intervals, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "print(f\"理论总采样点数: {total_points}\")\n",
    "print(f\"缺失点总数: {len(missing_points)}\")\n",
    "print(f\"缺失区间总数: {len(missing_intervals)}\")\n",
    "print(\"\\n缺失最多的前 10 个区间:\")\n",
    "for start, end, length in missing_intervals_sorted:\n",
    "    print(f\"point_{start} ~ point_{end}  (缺失 {length} 个)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd6fdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "理论总采样点数: 34600\n",
      "缺失点总数: 11172\n",
      "缺失区间总数: 1368\n",
      "\n",
      "缺失最多的前 10 个区间:\n",
      "point_17438 ~ point_23999  (缺失 6562 个)\n",
      "point_5743 ~ point_5781  (缺失 39 个)\n",
      "point_12032 ~ point_12068  (缺失 37 个)\n",
      "point_12555 ~ point_12585  (缺失 31 个)\n",
      "point_12133 ~ point_12158  (缺失 26 个)\n",
      "point_16269 ~ point_16294  (缺失 26 个)\n",
      "point_5597 ~ point_5621  (缺失 25 个)\n",
      "point_10183 ~ point_10207  (缺失 25 个)\n",
      "point_10698 ~ point_10720  (缺失 23 个)\n",
      "point_16435 ~ point_16457  (缺失 23 个)\n"
     ]
    }
   ],
   "source": [
    "#====================修改一下，针对人工提取每张图json合成的代码=========================#\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# 输入 JSON 文件路径\n",
    "input_json = r\"E:\\Dissertation\\XGBoost_cleaning\\2024Central_downloaded_merge_summary.json\"\n",
    "\n",
    "# 总采样点数\n",
    "total_points = 34600  \n",
    "\n",
    "# 加载 JSON\n",
    "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 统计每个 point 的图片数量\n",
    "point_counts = defaultdict(int)\n",
    "for item in data:\n",
    "    fname = item.get(\"file_name\", \"\")\n",
    "    match = re.search(r\"(point_\\d+)\", fname)\n",
    "    if match:\n",
    "        pid = match.group(1)\n",
    "    else:\n",
    "        pid = \"UNKNOWN\"\n",
    "    point_counts[pid] += 1\n",
    "\n",
    "# 已出现的点（转为 int，方便排序和区间分析）\n",
    "present_points = set(int(pid.split(\"_\")[1]) for pid in point_counts.keys() if pid.startswith(\"point_\"))\n",
    "\n",
    "# 所有应有点 ID\n",
    "all_points = set(range(1, total_points + 1))\n",
    "\n",
    "# 缺失点（完全没有出现的点）\n",
    "missing_points = sorted(all_points - present_points)\n",
    "\n",
    "# 计算缺失区间\n",
    "missing_intervals = []\n",
    "if missing_points:\n",
    "    start = missing_points[0]\n",
    "    prev = missing_points[0]\n",
    "    for p in missing_points[1:]:\n",
    "        if p == prev + 1:\n",
    "            prev = p\n",
    "        else:\n",
    "            missing_intervals.append((start, prev, prev - start + 1))  # (起点, 终点, 缺失数量)\n",
    "            start = p\n",
    "            prev = p\n",
    "    missing_intervals.append((start, prev, prev - start + 1))  # 收尾\n",
    "\n",
    "# 找出缺失最多的前 10 个区间\n",
    "missing_intervals_sorted = sorted(missing_intervals, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "print(f\"理论总采样点数: {total_points}\")\n",
    "print(f\"缺失点总数: {len(missing_points)}\")\n",
    "print(f\"缺失区间总数: {len(missing_intervals)}\")\n",
    "print(\"\\n缺失最多的前 10 个区间:\")\n",
    "for start, end, length in missing_intervals_sorted:\n",
    "    print(f\"point_{start} ~ point_{end}  (缺失 {length} 个)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 记录日志 =====\n",
    "def append_log(task_name, dataset, code_version, input_dir, output_dir, status, duration, note):\n",
    "    repo_dir = 'E:\\Dissertation\\CASA0004'\n",
    "    log_path = f\"{repo_dir}/operation_log.md\"\n",
    "\n",
    "    # 写入日志\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"**任务名称**: {task_name}\\n\")\n",
    "        f.write(f\"**数据集**: {dataset}\\n\")\n",
    "        f.write(f\"**代码版本**: {code_version}\\n\")\n",
    "        f.write(f\"**输入目录**: {input_dir}\\n\")\n",
    "        f.write(f\"**输出目录**: {output_dir}\\n\")\n",
    "        f.write(f\"**状态**: {status}\\n\")\n",
    "        f.write(f\"**耗时**: {duration}\\n\")\n",
    "        f.write(f\"**备注**: {note}\\n\\n\")\n",
    "\n",
    "    print(\"✅ 日志写入完成\")\n",
    "status=\"finished\"\n",
    "append_log(task_name, dataset, code_version, input_dir, output_dir, status, duration, note)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
